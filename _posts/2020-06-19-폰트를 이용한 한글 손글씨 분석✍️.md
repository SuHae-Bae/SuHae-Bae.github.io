---
layout: post
title: 폰트를 이용한 한글 손글씨 분석
date: 2020-06-19 00:00:00 +0300
description: 폰트를 이용한 한글 손글씨 분석 소개 # Add post description (optional)
img: handwriting.jpg # Add image post (optional)
tags: [모델링, 신경망, python, tensorflow] # add tag 
---


# 폰트를 이용한 한글 손글씨 분석✍️

- [깃허브 링크](https://github.com/SuHae-Bae/deeplearn_hangulhandwritten)



## 개요

딥러닝에 사용하는 손글씨 데이터는 굉장히 방대한데, Mnist의 숫자 손글씨 데이터, Nist의 영어 손글씨 등 그 종류와 수가 다양한 것이 특징이다. 그러나, 한글 손글씨 데이터는 영어나 숫자, 중국어 데이터에 비해 찾아보기 어렵고, 관련 논문이나 연구도 드물다. 이에 한글 손글씨 이미지 데이터를 수집하고, 이미지를 학습시켜 인식하도록 하였다.



## 삽질의 역사...

원래 AIHub의 데이터를 사용하려 했으나, 라벨 정보가 이미지 정보와 맞지 않아 폰트로 데이터를 생성하는 방식으로 변경했다. AIHub의 손글씨 데이터 예시를 들자면 다음과 같다.

![image](https://user-images.githubusercontent.com/33304926/147649027-80506e7d-9fbc-427a-945b-38c00e2cc547.png)

AIHub의 손글씨 이미지 데이터는 2번(00000002)부터 시작하지만, 라벨의 index는 0부터 시작한다. 앞에 0이 붙은 8자리 숫자는 손글씨 이미지 데이터를 나타내며, 라벨의 index는 0을 떼어낸 7자리 숫자에서 2를 뺀 값이다.  

때문에 가령 왼쪽에 있는 사진은 01102243이므로, 오른쪽에 있는 라벨 index인 1102241에 해당해야 했다. 하지만, 해당 텍스트를 출력해본 결과 "이제야"가 출력되지 않고 "펦"이 출력되었고 사진과 라벨이 정확하지 않음을 확인할 수 있었다. 둘이 일치하는 것도 있기는 있었지만, 이처럼 사진의 번호와 라벨 index가 맞지 않는 데이터가 다수 존재하였다.

이에 따라 부득이하게 폰트를 이용하여 이미지 데이터를 생성하는 방식으로 변경하게 되었다. 네이버 소프트웨어에서 다운받을 수 있는 글꼴(ttf)로 글자 이미지 데이터를 생성하였다. 내가 사용한 글꼴은 다음과 같은 총 7가지의 글꼴이다.

![image](https://user-images.githubusercontent.com/33304926/147650380-b94578c4-2ed4-4fcb-b0a6-f5a442529316.png)



## 데이터 생성

글자 이미지 데이터를 만들 때 사용한 음절은 국립국어원에 의해 편집된 기본 어휘 6000단어 목록을 바탕으로 만들어진 512개의 음절이다. 이 음절을 이용하여 글자 데이터와 라벨을 생성했는데, 나는 컴퓨터 성능의 문제로 64x64 크기의 JEPG 데이터 20480개를 생성하였다. 만약 데이터를 더 생성하고 싶거나 여건이 된다면, 음절을 늘리면 된다. 자세한 생성 과정은 다음과 같다. 깃허브에 올려져 있는 파일을 기준으로 설명하도록 하겠다.

우선 font폴더에 폰트가 받아져있는지 확인해보아야 한다. 원하는 폰트가 있다면 그걸 이용해도 된다. 단, ttf형식의 파일이어야 한다.

해당 데이터 생성기는 512개의 음절을 만들어낸다. 컴퓨터의 성능이 좋다면 음절을 더 추가하여 만들 수 있다. 음절을 추가할수록 생성되는 글자 데이터 양이 증가한다.

준비가 완료되면 로컬에서 해당 명령어를 실행시켜준다. 더 자세하게 알고싶다면 [여기](https://github.com/IBM/tensorflow-hangul-recognition)를 참고하길 바란다. 나 또한 이곳의 코드를 사용하였다.

```
python ./tools/hangul-image-generator.py
```

이후 image-data 폴더에 들어가면 생성된 이미지가 들어있는 폴더와 라벨 정보 및 이미지 위치가 들어있는 csv파일이 생성된다. 이 csv파일에서 ctr+f로 `\`를 모두 `/`로 바꾸어 준다.

이 작업이 끝나게 되면 다음과 같은 이미지들이 생성된 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/33304926/147650981-9be3fc8d-b98d-4fa8-b14c-89244971181d.png)



## 데이터 정제

데이터를 생성할 때, 이미지의 위치와 라벨정보가 든 CSV 파일이 함께 생성된다. 그런데 이 파일에는 /가 ₩로 표ㅅ기된 데이터가 존재하기 때문에, 이를 모두 /로 변경해 주어야 한다. 액셀의 바꾸기 기능이면 쉽게 변경 가능하다.

![image](https://user-images.githubusercontent.com/33304926/147661778-19a83ca9-44ab-4c80-8903-ce76b35dc8a5.png)

만약 한글이 보이지 않을 경우, 인코딩을 utf-8에서 utf-8-sig로 변경해준다.



## 데이터 분석

#### 데이터 분할

먼저, 더 빠른 데이터 분석을 위해 array형태로 데이터를 바꿔준 다음 colab에서 이후 코드를 실행한다(GPU사용을 위함).

```python
import numpy as np

X = np.array(X_data)
y = np.array(labels)

import pickle

# colab에 쉽게 데이터를 올려서 사용하기 위함
# 로컬에서 실행시킬 경우 그냥 np.array한 데이터를 여기서 사용하면 됨
np.save('train_data', X)
np.save('train_label', y)
```

이미지 데이터와 라벨을 로드하여 배열 형태로 바꾸어 준 다음, 데이터를 train, validation, test로 분할해두었다.

- test 데이터: 전체 데이터의 20%
- validation 데이터: test 데이터를 제외한 나머지의 20%
- train 데이터: 나머지 데이터 전체

![image](https://user-images.githubusercontent.com/33304926/147662371-0eb6695a-dda5-409c-88ba-a9bef3b0676f.png)



#### 분석 조건 설정 및 데이터 증폭

이미지를 생성할 때, 64x64의 데이터 이미지를 생성하였기 때문에 batch_size를 64로 설정했다.

epoch 횟수: 100회

총 512개의 음절을 사용하기 때문에 num_classes는 512로 설정해주었다.

```python
# 64*64의 이미지
batch_size = 64
epochs = 100

# 512개의 음절
num_classes = 512
```

충분한 데이터 수를 확보하기 위해 ImageDataGenerator을 사용

```python
datagen = ImageDataGenerator(
        rotation_range=20,
        height_shift_range=0.2,
        width_shift_range=0.2,
        horizontal_flip=True,
        vertical_flip=True)
```



#### 모델 생성 및 실행

모델을 만들 때, 먼저 컨볼루션 레이어를 넣고, 그 뒤 BatchNormalization을 진행하고 activation function을 실행한 뒤 Dropout을 넣는 것이 좋다는 논문을 봐서 이를 참고하여 레이어를 구성함. Dropout은 0.2로 설정하고, activation function으로는 ReLU를 사용하였다.[참고1](https://arxiv.org/pdf/1502.03167.pdf), [참고2](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout).

```python
model = Sequential()
model.add(Conv2D(64, (3, 3), padding='same',
                 input_shape=X_train.shape[1:]))
model.add(BatchNormalization())
model.add(Conv2D(64, (5, 5), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(512))
model.add(Dense(512, activation='softmax'))
```

![image](https://user-images.githubusercontent.com/33304926/147663297-7e43f417-fe15-4768-89e9-a7a0f9d3546e.png)

optimizer로는 RMSpop을 사용했는데, 이게 신경망 분석에 가장 많이 사용된다고 keras공식 문서에 나와있어 선택하였다.

```python
opt = keras.optimizers.RMSprop(lr=0.001, decay=0.0)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

# train
history = model.fit(datagen.flow(X_train, y_train, batch_size = batch_size), 
            steps_per_epoch=len(X_train)/batch_size,
            epochs=epochs,
            validation_data = (X_val, y_val),
            workers=4)
```

![image](https://user-images.githubusercontent.com/33304926/147663563-04a0bb29-107b-4c91-8a75-ccac8a8ddfc9.png)



## 분석 결과

모델을 학습시킨 결과를 python의 matplotlib을 이용하여 표현해보았다.

epoch에 따른 train 데이터의 accuracy와 validation 데이터의 accuracy를 그래프로 그려보았다.

```python
# validation accuracy와 accuracy를 비교하여 그래프로 표현해봄
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.1, 1])
plt.legend(loc='lower right')
```

![image](https://user-images.githubusercontent.com/33304926/147663790-de37f99d-399c-404c-85b6-47c28b07987f.png)

그래프를 보면, train도 validation도 70%에 가까운 것을 알 수 있다.

test 데이터에 모델을 적용시켜 예측을 진행한 결과, 약 72%의 정확도를 기록하였다.

```python
# test 데이터에 모델을 적용시켜 예측을 진행했을 때의 accuracy를 알아보게 함
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
```

![image](https://user-images.githubusercontent.com/33304926/147663903-eb086613-e04e-4bea-8f24-3d22ebc523cf.png)



## 정리 및 보완점

예시로 참고한 자료에서는 918개의 음절을 사용하여 epoch 300을 주어 약 85%의 accuracy를 기록하였다. 이 프로젝트에선 512개의 음절을 사용하여 epoch 100을 주어 약 72%의 accuracy를 기록하였다. 비록 비교 예시보다는 정확도가 낮게 나왔으나, epoch수와 데이터 종류를 더 늘렸다면 비슷한 결과 혹은 더 나은 결과가 나왔을 것이라 예상된다.



## 참고문헌

- Graham, Benjamin, "Spatially-spase convolutional neural networks." arXiv preprint arXiv:1409.6070 (2014).
- loffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." arXiv preprint arXiv: 1502.03167 (2015) K. Elissam "Title of paper if known," unpublished.
- IBM, pvaneck. https://github.com/IBM/tensorflow-hangul-recognition
- MijeongJeon, https://github.com/MijeongJeon/KoreanClassification_Keras_Coreml.
- https://keras.io/ko/optimizers/
- https://www.topikguide.com/download/6000_korean_words.htm